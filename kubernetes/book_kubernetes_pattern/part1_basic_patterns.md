## 2장 예측 범위 내 요구사항
### 문제
- 애플리케이션마다 요구사항이 다르다. 개발자가 테스트를 수행한 후에야 비로소 서비스 구현을 위한 자원 필요량을 알 수 있다. 
  - 순간적으로 많은 CPU를 요구
  - 영구 스토리지를 요구
- 자원 요구 사항 외에도 애플리케이션 런타임은 데이터 스토리지 또는 애플리케이션 설정 같은 플랫폼 관리 기능이 필요하다.

### 해결책
- 런타임 의존성이 정의되고 자원 요구사항이 계산되면 가장 효율적으로 컨테이너를 배치할 수 있다.
- 특별한 서비스 요구사항과 총 서비스 수에 근거해 용량 계획을 세울 수 있다.
- 그러므로 쿠버네티스에서 런타임 의존성을 선언하는 다양한 방법들을 알아두어야 한다.

### 런타임 의존성
- 파일 스토리지
  - volume을 사용하여 영구 스토리지 할당 가능
  - persistentVolumeClaim으로 pv를 요청할 수 있으며, 이 때 클러스터가 제공해 줄 수 없는 볼륨일 경우 파드는 스케쥴링 되지 않는다.
- 설정
  - configMap, secret 등으로 설정 값을 주입할 수 있다.
  - 이를 volume을 활용해 파일 형태로 전달할 수도 있다.

### 자원 프로파일
- 압축 가능 자원: CPU, 사용 가능량을 넘어서서 사용하면 throttle이 걸린다.
- 압축 불가 자원: 메모리, 사용 가능량을 넘어서면 컨테이너가 죽어버림
- 최소량과 최대량을 설정해주어야함.

### 파드 서비스 품질 (Quality of Service)
- 노드의 압축 불가 자원에 대한 압박이 있을 경우 kubelet은 파드의 서비스 품질 우선 순위에 따라서 어떤 파드를 죽일지 결정한다.
- Best Effort 파드
  - requets, limit 갖고 있지 않다.
  - 가장 낮은 우선순위
  - 파드가 위치한 노드의 압축 불가 자원이 전부 사용되면 가장 먼저 죽음
- Burstable 파드
  - request, limit 모두 가지며, limits 값이 더 크다.
  - 노드가 압축 불가 자원에 대한 압박을 받는 경우 Best Effort 파드 다음 우선 순위로 죽는다.
- Guaranteed 파드
  - request와 limit가 동일하다.
  - 노드가 압축 불가 자원에 대한 압박을 받는 경우, 가장 나중에 죽는다.

### 파드 우선순위
- 파드의 중요성을 명시적으로 지정할 수 있으며, 이는 kube-scheduler가 어떤 파드를 먼저 스케쥴링 할 지 결정한다.
```
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
glovalDefault: false
---
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: nginx
    name: mypod
  priorityClassName: high-priority
```
- kube-scheduler는 스케쥴링 시에 우선순위가 가장 높은 파드가 먼저 오도록 큐를 정렬한다.
- 만일 클러스터에 충분한 용량을 가진 노드가 없다면 우선 순위가 낮은 파드를 죽이고 우선순위가 높은 파드를 스케쥴링한다.
- 스케쥴러는 파드의 서비스 품질은 아예 신경쓰지 않는다.

### 프로젝트 자원
- 네임 스페이스 안에서 소비할 수 있는 최대 자원량을 설정해 줄 수 있다.
- LimitRange를 설정하면 각 자원 종류마다 최대 자원량을 설정할 수 있다.

### 정리
- 자원 프로파일이 식별되면 컨테이너는 성공적인 용량 계획을 위한 구성 요소가 된다. 
- 초기 테스트를수행해 각 컨테이너에 필요한 자원을 찾고 이를 향후 용량 계획 및 용량 예측의 기본 정보로 사용하라.
- request와 limit을 제공하지 않은 컨테이너는 클러스터가 가득 차면 죽게 된다.

## 3장 선언적 배포
### 문제
- 선언적 배포의 핵심은 deployment이다. 이는 컨테이너 그룹의 업그레이드 및 롤백 프로세스를 캡슐화한다.
- 마이크로서비스의 수가 증가하면 새로운 버전을 지속적으로 업데이트하고 교체하는 것도 부담이 된다.
- 새로운 버전의 파드를 시작하기, 이전 버전의 파드를 안전하게 중지하기, 새로운 파드가 성공적으로 시작되었는지 대기 및 확인하기, 실패시 롤백하기 등의 기능이 필요

### Rolling Update
- maxSurge, maxUnavailable을 설정하여 한번에 새로 추가되는 파드의 수와 종료되는 파드의 수를 설정한다.
- 업데이트 시에 deployment는 새로운 replicaset을 만들어 새로운 파드를 추가하고 기존 레플리카 셋의 파드를 종료한다.
- kubectl replace로 새로운 버전의 디플로이먼트로 교체, kubectl edit, kubectl set image 등으로 rolling update를 수행할 수 있다.
- 모든 업데이트 과정은 기록되며 롤 백 할 수 있다.

### Recreate
- 싹다 죽이고 새로운 버전의 파드를 일괄 생성하는 방법
- 다운타임이 발생

### blue green 배포
- 서비스 메시나 Knative와 같은 확장 서비스를 사용하지 않는다면 수동으로 진행해야한다.
- 새로운 버전의 파드 준비가 완료되면 트래픽을 전환하는 방식이다.
- 컨테이너 용량이 2배로 필요하다는 단점이 있다.

### 카나리 배포
- 인스턴스의 작은 하위 집합만 새로운 인스턴스로 교체함으로써 새로운 버전의 애플리케이션 운영에 유연하게 배포하는 방식
- 운영에 새 버전을 도입할 때 위험을 줄여준다.
- 새로운 버전의 서비스와 소수 사용자 시험군에게 적용된 방식이 만족스럽다면 이전 인스턴스를 모두 교체한다.

## 4장 정상상태 점검
### 문제
- 쿠버네티스는 컨테이너 프로세스 상태를 주기적으로 확인하고 문제가 감지되면 컨테이너를 다시 시작한다.
- 그러나 프로세스 상태 확인은 애플리케이션 정상 상태를 결정하기에 충분하지 않다.
- 예를들어 애플리케이션이 hang이 걸려도 프로세스 자체는 여전히 실행 중이다.
- 자바 애플리케이션의 경우 OutOfMemoryError를 던지더라도 여전히 JVM 프로세스는 실행 중일 수 있다.
- 혹은 무한 루프, 쓰래싱 등으로 인해 동작을 안할 수 있다.

### 프로세스 정상 상태 확인
- kubelet은 컨테이너 프로세스에 정상 상태인지 지속적으로 확인한다.

### livenessProbe
- 애플리케이션이 deadlock에 빠지면 프로세스 정상상태 확인으로는 여전히 정상으로 간주된다.
- 큐블릿이 정기적으로 검사를 수행해 컨테이너가 여전히 정상상태인지 확인하는 과정이다.
- http GET, TCP 소켓 점검, 스크립트 실행 등 세 가지 방법이 있다.
- livenessProbe를 통과하지 못한 컨테이너는 재시작 된다.

### readinessProbe
- 재시작이 컨테이너가 제대로 동작하기 위해 충분하지 않은 상황이 있을 수 있다.
- 컨테이너가 여전히 시작중이고 아직 요청을 처리할 준비가 되지 않았을 경우가 있다.
- 컨테이너에 과부하가 걸릴 수도, 지연 시간이 증가할 수도 있으며 , 또 다른 부하로부터 잠시 보호받길 원할 수 있다.
- 이 때 필요한 것이 readinessProbe이다. 이는 트래픽으로부터 서비스를 보호하는 데 유용하다.
- readinessProbe가 있으면 컨테이너는 시작할 시간을 벌 수 있다.

### 정리
- readinessProbe나 livenessProbe가 유용하지만 결국 컨테이너가 죽는 근본적인 원인을 알아내야한다.
- 따라서 컨테이너 종료 원인을 로그로 남기는 것이 좋다.

## 5장 수명주기 관리
### 문제
- 컨테이너의 정상 상태를 모니터링 하기 위해서는 프로세스 상태 뿐만 아니라 다른 기능 (readinessProbe, livenessProbe)이 필요했다.
- 마찬가지로 프로세스를 실행하고 중지하는 데에 프로세스 모델만 사용하는 것은 충분치 않다. 더 세밀한 수명주기 관리가 필요하다.
- 애플리케이션의 배포 단위는 파드이며 이는 하나 이상의 컨테이너로 구성된다.

### SIGTERM / SIGKILL
- 컨테이너가 속한 파드가 종료중이기 때문이든, 라이브니스 점검이 실패해 재시작되든 컨테이너를 멈추기 위해 SIGTERM을 보낸다.
- 이는 SIGKILL을 보내기 전에 컨테이너가 스스로 종료될 수 있도록 여유를 주며 30초가 기본값이다.
- 여유 시간이 지나도 동작 중이면 SIGKILL을 보내어 강제 종료한다.

### postStart / preStop
- 수명 주기 관리를 위해서 프로세스 신호만 사용하는 것은 제한적이다.
- 컨테이너 아래에 postStart와 preStop이라는 수명주기 훅을 설정할 수 있다.
```
spec:
  containers:
  - image:
    name:
    lifecycle:
      postStart:
        exec:
          command:
          - sh
          - -c
          - sleep 30 && echo "wake up!"
```
- postStart 명령어는 컨테이너가 생성된 후 비동기적으로 실행된다.
- postStart를 완료하기 전까지 컨테이너는 Waiting 상태로 대기하며 파드는 Pending
- 종료 직전에는 마찬가지로 preStop 생명주기가 발동한다.

## 6장 자동 배치
- 스케쥴러의 핵심 기능으로 kube-apiserver로부터 새로 생성된 파드의 정의를 조회하고 파드를 노드에 할당하는 것
- 새로운 파드를 실행하기 위한 충분한 자원 용량이 확보된 노드가 있어야한다.
- 가용 용량 = 전체 용량 - 쿠버네티스 시슨템 용량 - OS 시스템 용량이

### 배치 정책
- 직접 커스텀 스케쥴러를 만들고 스케쥴링 정책을 부여할 수 있다.
```
apiVersion: v1
kind: Policy
predicates: [
  {"name": "PodFitHostPorts"},
  ...
],
"priorities": [
  {"name": "LeastRequestedPriority", "weight": 2},
  ...
]
```

### 스케쥴링 프로세스
- 파드 요구 조건에 맞지 않은 노드들을 필터링한다.
- 남은 노드들의 가중치를 계산하여 정렬한다.
- 파드를 배치한다.
- 노드 셀렉터, 노드 어피니티, 파드 어피니티, 파드 안티 어피니티 등을 사용하여 특정한 노드에 배치되도록 조정할 수 있다.
- 노드 어피니티는 노드 셀렉터보다 더 다양한 operator 사용이 가능하다.
- 파드 어피니티의 경우 함께 배치되어야 하는 파드, 함께 배치되면 안되는 파드를 설정할 수 있다.
- taint와 toleration으로 특정 노드에 더 이상 파드가 스케쥴링 되지 않도록 설정할 수 있다.
- 스케쥴러의 결정은 파드 생성 시점의 클러스터 상태가 기준, 따라서 일정 시간이 지나게 되면 재배치 할 필요가 발생
